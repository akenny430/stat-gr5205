\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[margin = 0.75in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}



\titleformat{\section}{\normalsize\bfseries}{\thesection}{1em}{}
% \titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\setcounter{secnumdepth}{0}
% \numberwithin{equation}{section}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1 \\ }
\newcommand{\mycolaba}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1}

\title{
    {\Large Final Exam}
}
\author{
    {\normalsize Aiden Kenny}\\
    {\normalsize STAT GR5205: Linear Regression Models}\\
    {\normalsize Columbia University}
}
\date{\normalsize December 21, 2020}

\begin{document}

\maketitle

% \newcommand{\rx}{\mathcal{X}}
\newcommand{\rx}{X}
% \newcommand{\ry}{\mathcal{Y}}
\newcommand{\ry}{Y}
% \newcommand{\rz}{\mathcal{Z}}
\newcommand{\rz}{Z}
\newcommand{\E}{\mathbb{E}}
% \newcommand{\E}{\mathrm{E}}

%' ============================================================================================================================================================
\section{Question 1} \noindent
Let \(\ry\) denote per-capita gross metropolitan product (GMP), in dollars per person per year, and \(\rx\) denote population, in people. 
The realized values of these random variables are respectively given by the \(n\)-vectors \(\mathbf{y}\) and \(\mathbf{x}\), where \(n = 366\).
\begin{enumerate}
    \item The predictor variable is given by \(\rz \coloneqq \log_{10}\rx\), and the response is \(\ry\). We can see that the population is being transformed 
    by taking the logarithm (with base 10). 
    \item Our estimated model is given by 
    \begin{align}
        % \hat{\ry}
        \E(\ry \,|\, x)
        % \mathbb{E}[\ry \,|\, \rx]
        % = -23306 + 10246 \rz
        = -23306 + 10246 \log_{10} x.
    \end{align}
    \item We have \(\E(\ry \,|\, 1,000,000) = 38170\) and \(\E(\ry \,|\, 200,000) = 31008.35\).
    These answers make sense, a city with a larger population will have a higher GMP per-capita. \\[0.5em]
    \texttt{-23306 + 10246 * log(c(1000000, 200000), 10)}
    \item We cannot give an estimate of \(\E(\ry \,|\, 0)\) because \(\log_{10} 0\) is undefined.
    \item A \(95\%\) confidence interval for \(\beta_1\), denoted as \(\mathcal{I}_{\beta_1}\), is
    \begin{align}
        \mathcal{I}_{\beta_1}
        = \left( \hat{\beta}_1 - t \cdot \mathrm{se}(\hat{\beta}_1), \hat{\beta}_1 + t \cdot \mathrm{se}(\hat{\beta}_1) \right)
        = \big( 10246 - 1.967 \cdot 900, 10246 + 1.967 \cdot 900 \big)
        = \big( 8475.7, 12016.3 \big).
    \end{align}
    The values \(\hat{\beta}_1 = 10246\) and \(\mathrm{se}(\hat{\beta}_1) = 900\) can be found in the \texttt{R} output, and the value 
    \(t = T_{364}^{-1}(0.975) = 1.967\) can be found using the \texttt{qt()} function in \texttt{R}. \\[0.5em]
    \texttt{qt(0.975, 364)}\\[0.5em]
    \texttt{10246 + 1.967 * 900 * c(-1, 1)}
    \item From the \texttt{\#{}\#{}Residual standard error} section, we have \(\hat{\sigma}^2 = (7930/364)^2 = 474.6173\). \\[0.5em]
    % \texttt{7930\textasciicircum{}2}
    \texttt{(7930 / 364)\textasciicircum{}2}
    \item You cannot find the sample variance of \(\rx\) from the \texttt{R} output. We are never considering the value of 
    \(\mathrm{Var}(\rz)\) when constructing the model because we are never treating \(\rz\) as a random variable. We instead are treating it as 
    a set of fixed values \(\mathbf{z}\), either observed before or after the model's design is chosen. When we estimate \(\sigma^2\) in 
    the linear model, we are estimating \(\mathrm{Var}(\epsilon)\), the residuals of the model. And since we cannot make any inferences about 
    \(\mathrm{Var}(\rz)\), we cannot make any inferences about \(\mathrm{Var}(\rx)\) either. 
    % ; practically \(\mathbf{z}\) is either the observed values of the transformed predictor variable before fitting the model, 
    % or the conditional values of the random variable \(\ry \,|\, \mathbf{z}\). 
    \item There are multiple components of the \texttt{R} output that test the hypothesis \(H_0 : \beta_1 = 0\) against \(H_A : \beta_1 \neq 0\). 
    Remember, the output is testing the hypothesis that \(\ry\) and \(\rz\) have a linear relationship, \textsl{not} \(\ry\) and \(\rx\). 
    There are two tests that \texttt{R} runs when using the \texttt{lm()} function: the \(t\) test and the ANOVA test. The \(p\)-palue for the \(t\) test
    is found in the right-most column, \texttt{Pr(>|t|)}, of the \texttt{\#{}\#{}Coefficients} section, and is given by \texttt{<2e-16}. 
    The \(p\)-value for the ANVOA test is found in the last entry in the output, in the \texttt{\#{}\#{}F-statistic} section, and is also given by \texttt{<2e-16}
    (\texttt{R} will estimate the value if it is too small). In both cases, we reject \(H_0\), and it seems that there is indeed a linear relationship 
    between \(\ry\) and \(\rz\) (\(=\log_{10}\rx\)). 
\end{enumerate}

%' ============================================================================================================================================================
\newcommand{\mx}{\mathbf{X}}
\newcommand{\ax}{\widetilde{\mathbf{X}}}
\section{Question 2} \noindent
Suppose we have observed \(n\) observations of \(p\) predictors, given by \(\mathbf{x}_1, \ldots, \mathbf{x}_p\), and an observed response \(\mathbf{y}\).
Let \(\mathbf{X} \coloneqq \begin{bmatrix}
    \mathbf{x}_1 \cdots \mathbf{x}_p
\end{bmatrix}\) be a matrix where the \(j\)th column is \(\mathbf{x}_j\).
Here we also assume that the data has been centered, so each predictor and the response has a mean of zero (this is common to do before fitting a model).
We fit a regression model \(\mathbf{y} = \mathbf{X}\bm{\beta} + \bm{\epsilon}\), and our estimated coefficients are given by 
\(\hat{\bm{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\). 

% Now, suppose we have \(n\) observations of a new predictor \(\mathbf{x}_{p+1}\)
Now, suppose we have \(n\) observations of a new predictor \(\mathbf{z}\), which was not used at all when determining \(\hat{\bm{\beta}}\). 
It turns out that this new predictor is orthogonal to each of the previous \(p\) predictors, i.e. \(\mathbf{x}_j^T\mathbf{z} = 0\) for all \(j\), 
and so \(\mathbf{X}^T\mathbf{z} = \mathbf{0}\). If we want to fit a new linear model that includes \(\mathbf{z}\), we can use an alternate 
matrix \(\ax \coloneqq \begin{bmatrix}
    \mathbf{X} & \mathbf{z}
\end{bmatrix}\) and fit the model \(\mathbf{y} = \ax \bm{\beta}\); our estimated coefficient will be given by \(\hat{\bm{\beta}} = (\ax^T\ax)^{-1}\ax\mathbf{y}\).
As we will soon see, the estimated coefficients for the original \(p\) predictors is exactly the same as they were in the origianl model (denoted as \(\hat{\bm{\beta}}_0\)), 
and the estimated coefficient for \(\mathbf{z}\) would be the same if a model was fit using \textsl{only} this new predictor!
The key reason for both of these results is that the new predictor is orthogonal to each of the previous ones. 
To see this, we first observe that 
\begin{align*}
    \ax^T\ax
    = \begin{bmatrix}
        \mathbf{X}^T \\ \mathbf{z}^T
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{X} & \mathbf{x}
    \end{bmatrix}
    = \begin{bmatrix}
        \mathbf{X}^T\mathbf{X} & \mathbf{X}^T\mathbf{z} \\
        \mathbf{z}^T \mathbf{X} & \mathbf{z}^T\mathbf{z}
    \end{bmatrix}
    = \begin{bmatrix}
        \mathbf{X}^T\mathbf{X} & \mathbf{0} \\
        \mathbf{0}^T & \mathbf{z}^T\mathbf{z}
    \end{bmatrix}.
\end{align*}
Using the formula for matrix inversion for block matrices (see \href{https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion}{\texttt{here}}), we have 
\begin{align*}
    (\ax^T\ax)^{-1}
    = \begin{bmatrix}
        \mathbf{X}^T\mathbf{X} & \mathbf{0} \\
        \mathbf{0}^T & 1 / \mathbf{z}^T\mathbf{z}
    \end{bmatrix},
\end{align*}
and so the estimated coefficients are given by 
\begin{align}
    \hat{\bm{\beta}}
    = (\ax^T\ax)^{-1}\ax\mathbf{y}
    = \begin{bmatrix}
        \mathbf{X}^T\mathbf{X} & \mathbf{0} \\
        \mathbf{0}^T & 1 / \mathbf{z}^T\mathbf{z}
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf{X}^T \\ \mathbf{z}^T
    \end{bmatrix}
    \mathbf{y}
    = \begin{bmatrix}
        (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \\
        \mathbf{z}^T\mathbf{y} / \mathbf{z}^T\mathbf{z}
    \end{bmatrix}
    = \left( \hat{\bm{\beta}}_0, \frac{\mathbf{z}^T\mathbf{y}}{\mathbf{z}^T\mathbf{z}} \right)^T.
\end{align}
This result says that the first \(p\) estimated coefficients are given by \(\hat{\bm{\beta}}_0\), while the new predictor's estimated coefficient is 
given by the value \(\mathbf{z}^T\mathbf{y} / \mathbf{z}^T\mathbf{z}\). If we fit a simple linear model using only the new predictor, 
\(\mathbf{y} = \beta \mathbf{z}\), the estimated coefficient is given by 
\(\hat{\beta} = (\mathbf{z}^T\mathbf{z})^{-1}\mathbf{z}^T\mathbf{y} = \mathbf{z}^T\mathbf{y} / \mathbf{z}^T\mathbf{z}\).

The two main points of this question are that adding an orthogonal variable to a linear model does not change the value of the estimated coefficients 
of the previous variables, and obtaining the estimated coefficient for the new variable is as easy as computing two inner products. 
In fact, these two ideas are a possible method for estimating \(\bm{\beta}\) for any linear model. One would first have to orthogonalize each of the 
variables to be used, and the estimated coefficient for the \(j\)th variable is then given by \(\mathbf{x}_j^T\mathbf{y} / \mathbf{x}_j^T\mathbf{x}_j\)
(see chapter 3 of \href{https://web.stanford.edu/~hastie/ElemStatLearn/}{\texttt{this book}} for more info).

\end{document}