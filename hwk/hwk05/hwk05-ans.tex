\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[margin = 1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}



% \titleformat{\section}{\normalsize\bfseries}{\thesection}{1em}{}
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\setcounter{secnumdepth}{0}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1 \\ }
\newcommand{\mycolaba}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1}

\title{
    {\Large Homework 5}
}
\author{
    {\normalsize Aiden Kenny}\\
    {\normalsize STAT GR5205: Linear Regression Models}\\
    {\normalsize Columbia University}
}
\date{\normalsize Novermber 25, 2020}

\begin{document}

\maketitle

\newcommand{\myf}{\mathbf{f}_{\lambda}}
\newcommand{\myffull}{\mathbf{f}_{\lambda}(\mathbf{y})}
\newcommand{\myg}{\mathbf{g}_{\lambda}}
\newcommand{\mygfull}{\mathbf{g}_{\lambda}(\mathbf{y})}
%' ============================================================================================================================================================
\section{Question 1} \noindent
\mycolaba{None}
\begin{itemize}
    \item[(a)] Let \(Y\) be the number of nurses in the hospital and let \(X\) be the available faculty and services. 
    The left and middle panels of Figure \ref{q01-investigation} show the histograms of \(Y\) and \(X\), respectively. We see that \(Y\) is skewed right, while 
    \(X\) appears to be normally-distributed. In addition, the scatterplot of \(Y\) vs. \(X\), which is in the third panel of Figure 
    \ref{q01-investigation}, shows that there is a nonlinear relationship between \(Y\) and \(X\). 
    All of these indicate that \(Y\) is suitable for a data transformation. Specifically, we would like to perform a 
    power transformation on \(Y\) in order to make it closer to a normal distribution. 
    \begin{figure}[ht]
        \centering
        \includegraphics[width = 0.32\textwidth]{img/q01-nurses-hist-original.png}
        \includegraphics[width = 0.32\textwidth]{img/q01-afs-hist-original.png}
        \includegraphics[width = 0.32\textwidth]{img/q01-scatterplot-blank.png}
        \caption{Histograms of \(Y\) and \(X\) and a scatterplot of \(Y\) vs. \(X\).}
        \label{q01-investigation}
    \end{figure}
    \item[(b)] The power transformation function and its scaled counterpart are defined as 
    \begin{align*}
        f_{\lambda}(Y) 
        = \begin{cases}
            % \frac{Y^{\lambda} - 1}{\lambda} & \text{if } \lambda \neq 0 \\
            \hfil Y^{\lambda} & \text{if } \lambda \neq 0 \\
            \log Y & \text{if } \lambda = 0.
        \end{cases}
        ~~~\text{and}~~~
        g_{\lambda}(Y) 
        = \begin{cases}
            % \frac{Y^{\lambda} - 1}{\lambda} & \text{if } \lambda \neq 0 \\
            (Y^{\lambda} - 1) / \lambda & \text{if } \lambda \neq 0 \\
            \hfil \log Y & \text{if } \lambda = 0.
        \end{cases}
        % g_{\lambda}^{-1}(Y) 
        % = \begin{cases}
        %     \left( 1 + \lambda Y \right)^{1 / \lambda} & \text{if } \lambda \neq 0 \\
        %      \hfil \mathrm{exp}(Y) & \text{if } \lambda = 0.
        % \end{cases}
    \end{align*}
    When transforming \(Y\), we first use the scaled power transform to fit the model \(g_{\lambda}(Y) = \beta_0 + \beta_1 X + \epsilon\) in order
    to determine an optimal value of \(\lambda\) via maximum likelihood estimation. 
    We then use \(f_{\lambda}\) to make \(Y\) closer to a normal distribution and perform any subsequent inferences. 
    % If we have our \(n\) observations (\(\mathbf{x}\),\(\mathbf{y}\)).
    In vector form, our model is \(\mygfull = \beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \bm{\epsilon}\), where \(\bm{\epsilon} \sim \mathrm{N}(\mathbf{0}, \sigma^2 \mathbf{I})\)
    and \(\mygfull\) is the transformed response (so \([\mygfull]_i = g_{\lambda}(y_i)\)) for some unknown \(\lambda\). 
    % Since \(\mygfull \sim \mathrm{N}(\beta_0 \mathbf{1} + \beta_1 \mathbf{x}, \sigma^2 \mathbf{I})\), 
    It can be shown (see Appendix A) that the log-likelihood function can be expressed as a function of only \(\lambda\), 
    \begin{align*}
        m(\lambda)
        \coloneqq - \frac{n}{2} \log \left( \frac{2 \pi \mathrm{e}}{n} \right) - \frac{n}{2} \log \Big( \myg^T(\mathbf{y})(\mathbf{I} - \mathbf{H}) \myg(\mathbf{y}) \Big) + (\lambda - 1) \sum_{i=1}^n \log(y_i),
    \end{align*}
    where \(\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\) is the hat matrix. \(m(\lambda)\) has been plotted in the left panel of Figure \ref{q01-box-cox},
    where we can see that the MLE is maximuzed at \(\lambda = 0.085\). 

    \begin{figure}[ht]
        \centering
        \includegraphics[width = 0.32\textwidth]{img/q01-box-cox-lambda.png}
        \includegraphics[width = 0.32\textwidth]{img/q01-scatterplot-power.png}
        \includegraphics[width = 0.32\textwidth]{img/q01-scatterplot-nurses.png}
        \caption{Relevant plots for the power transformation of \(Y\).}
        \label{q01-box-cox}
    \end{figure}

    \item[(c)] Now that we have our optimal \(\lambda\), we will fit the model \(f_{0.085}(Y) = \beta_0 + \beta_1 X + \epsilon\). 
    We have \(\hat{\beta}_0 = 1.255\) and \(\hat{\beta}_1 = 0.005953\). A plot of this model, along with a \(95\)\%{} confidence interval,
    has been printed in the middle panel of Figure \ref{q01-box-cox}. This means that when \(X\) increases by 1 unit, \(f_{0.085}(Y)\)
    is expected to increase by \(0.005953\) units. We can also make inferences about \(Y\) itself; we have 
    \(Y = f_{0.085}^{-1}(\hat{\beta}_0 + \hat{\beta}_1 X) = \big(1.255 + 0.005953 X)^{1 / 0.085}\). A plot of this model has been printed in 
    the right panel of Figure \ref{q01-box-cox}. 
    % This means that when \(X\) increases by one unit, \(Y\) is expected to increase by \(\big(1.255 + 0.005953)^{1 / 0.085} = 15.3\) units. 
    We must keep in mind that this model is non-linear, so it's expected rate of change will depend on the value of \(X\). 

    \item[(d)] Both diagnostic plots can be found in Figure \ref{q01-diagnostics}, and the data seems to confirm the model assumptions very well.
    \begin{figure}[ht]
        \centering
        \includegraphics[width = 0.4\textwidth]{img/q01-diagnostic-residuals.png}
        \quad
        \includegraphics[width = 0.4\textwidth]{img/q01-diagnostic-qqplot.png}
        \caption{Diagnostic plots for the power transformation model.}
        \label{q01-diagnostics}
    \end{figure}

    \item[(e)] We cannot conduct an \(F\)-test on the two models. This is because both models have the same number of parameters that need to be estimated,
    meaning they both have the same number of degrees of freedom. 

    \item[(f)] In order to find a prediction interval for \(Y\), we can first find a prediction interval for \(f_{0.085}(Y)\) and then apply \(f_{0.085}^{-1}\)
    to both ends of the interval. The prediction intervals for when \(X = 30\) and \(X = 60\) have been printed in Table \ref{q01-prediction-intervals}. 
    For reference, the prediction intervals at these observations for the linear model \(Y = \beta_0 + \beta_1 + \epsilon\) have also been printed. 
    The prediction intervals for the power model can also be seen in the middel and right plots of Figure \ref{q01-box-cox}. 
    For starters, the width of the intervals for the power model drastically increases as \(X\) increases, while the increase for the linear model is negligible. 
    When \(X = 30\), the confidence interval for the power model is significantly narrower than the linear function, but when \(X = 60\), the opposite is true. 
    % When \(X = 30\), using the power model we are \(95\)\%{} sure that the 
    That is, when \(X = 30\) we are confident that the next observation will lie in a smaller interval using the power model, but when \(X = 60\), the linear model 
    gives us the smaller region. Depending on the value of \(X\), I would use the better of the two models. 

    \begin{table}
        \def\arraystretch{1.25}
        \centering
        \begin{tabular}{|l|cccc|cccc|}
            \hline%
            &\multicolumn{4}{|c|}{\(X = 30\)} & \multicolumn{4}{|c|}{\(X = 60\)}\\
            \hline%
            Model & \(\hat{Y}\) & Lower & Upper & \(\Delta\mathcal{I}\) & \(\hat{Y}\) & Lower & Upper & \(\Delta\mathcal{I}\)\\
            \hline%
            Linear & 74.612 & \(-89.781\) & 239.004 & 328.785 & 297.769 & 133.036 & 462.503 & 329.467 \\
            Power  & 69.448 & 26.573      & 168.796 & 142.223 & 276.313 & 117.843 & 611.638 & 493.795 \\
            \hline%
        \end{tabular}
        \caption{Comparing the prediction intervals of the linear model against the power model.}
        \label{q01-prediction-intervals}
    \end{table}
\end{itemize}

%' ============================================================================================================================================================
\section{Question 2} \noindent
\mycolaba{None}


%' ============================================================================================================================================================
\section{Appendix A} \noindent

In this section, we will derive the result for \(m(\lambda)\), the log-likelikelihood as a function of only \(\lambda\).
Suppose we have our response vector \(\mathbf{y}\) and our observed data \(\mathbf{X} = \begin{bmatrix}
    \mathbf{1} & \mathbf{x}
\end{bmatrix}\). 
We are interested in fitting the model \(\mathbf{g}_{\lambda}(\mathbf{y}) = \mathbf{X}\bm{\beta} + \bm{\epsilon}\), where \(\mathbf{g}_{\lambda}(\mathbf{y})\)
is the transformed response vector (i.e. the \(i\)th element is given by \(g_{\lambda}(y_i)\)), \(\mathbb{E}[\bm{\epsilon}] = \mathbf{0}\), and 
\(\mathrm{Var}[\bm{\epsilon}] = \sigma^2 \mathbf{I}\). 
For notational ease, we will denote \(\mygfull\) as \(\myg\). 
If we make the further assumption that \(\bm{\epsilon}\) is normally distributed, i.e. 
\(\bm{\epsilon} \sim \mathrm{N}(\mathbf{0}, \sigma^2 \mathbf{I})\), then our response vector is also normally distributed, where
\(\myg \sim \mathrm{N}(\mathbf{X}\bm{\beta}, \sigma^2 \mathbf{I})\). It's density function (and thus it's likelihood function) is given by 
\begin{align*}
    % f(\myg \,|\, \mathbf{X}, \bm{\beta}, \sigma^2, \lambda)
    f(\myg \,|\, \bm{\beta}, \sigma^2, \lambda)
    % f(\myg)
    &= \frac{1}{\sqrt{\mathrm{det}(2 \pi \sigma^2 \mathbf{I})}} \cdot \mathrm{exp} \left( - \frac{(\myg - \mathbf{X}\bm{\beta})^T \big( \sigma^2 \mathbf{I} \big)^{-1} (\myg - \mathbf{X}\bm{\beta})}{2} \right) \\
    &= \frac{1}{(2\pi\sigma^2)^{n/2}} \cdot \mathrm{exp} \left( - \frac{(\myg - \mathbf{X}\bm{\beta})^T (\myg - \mathbf{X}\bm{\beta})}{2 \sigma^2} \right).
\end{align*}
Since \(\mathbf{y}\) is a transformation of \(\myg\) (via \(g_{\lambda}^{-1}\)), we can derive the density for \(\mathbf{y}\) as well. Notationally, this result may be somewhat 
confusing; even though we are finding the density for \(\mathbf{y}\), we will still express the density (partly) in terms of \(\myg\). It is important
to remember that \(\myg\) is a function of \(\mathbf{y}\). Because the \(i\)th element of \(\myg\) only depends on the \(i\)th element of \(\mathbf{y}\), the Jacobian
will be a diagonal matrix, and so 
\begin{align*}
    \mathbf{J}
    = \frac{\partial \myg}{\partial \mathbf{y}}
    = \mathrm{diag} \left( \frac{\partial g_{\lambda}(y_1)}{\partial y_1}, \ldots, \frac{\partial g_{\lambda}(y_n)}{\partial y_n} \right)
    = \mathrm{diag} \big( y_1^{\lambda - 1}, \ldots, y_n^{\lambda - 1} \big), 
\end{align*}
and so the density (and thus the likelihood) of \(\mathbf{y}\) is given by 
\begin{align*}
    g(\mathbf{y} \,|\, \bm{\beta}, \sigma^2, \lambda)
    = f \big( \myg(\mathbf{y}) \big) \cdot \big|\mathrm{det}(\mathbf{J})\big|
    = \frac{1}{(2\pi\sigma^2)^{n/2}} \cdot \mathrm{exp} \left( - \frac{(\myg - \mathbf{X}\bm{\beta})^T (\myg - \mathbf{X}\bm{\beta})}{2 \sigma^2} \right) \cdot \prod_{i=1}^n y_i^{\lambda - 1}.
\end{align*}
The log-likelihood \(\ell(\mathbf{y}) = \log g(\mathbf{y})\) is given by 
\begin{align*}
    \ell(\mathbf{y} \,|\, \bm{\beta}, \sigma^2, \lambda) 
    = - \frac{n}{2} \log (2\pi) - \frac{n}{2} \log (\sigma^2) - \frac{(\myg - \mathbf{X}\bm{\beta})^T (\myg - \mathbf{X}\bm{\beta})}{2 \sigma^2} + (\lambda - 1) \sum_{i=1}^n \log (y_i).
\end{align*}
As is standard with maximum likelihood estimation, we now differentiate \(\ell\) with respect to the unknown parameters, set the derivatives to zero, and solve to get the maximum value of \(\ell\). 
For now, we are going to leave \(\lambda\) fixed and differentiate with respect to \(\bm{\beta}\) and \(\sigma^2\). Doing this for both gives us 
\(\hat{\bm{\beta}}_{\mathrm{MLE}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\myg\) and \(\hat{\sigma}^2_{\mathrm{MLE}} = \myg^T (\mathbf{I} - \mathbf{H}) \myg / n\), where 
\(\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\) is the hat matrix. 
It is worth noting that both \(\hat{\bm{\beta}}_{\mathrm{MLE}}\) and \(\hat{\sigma}^2_{\mathrm{MLE}}\) are functions of \(\lambda\). 
Plugging these values back into \(\ell\) will maximize it with respect to \(\bm{\beta}\) and \(\sigma^2\), 
which means we will only have to maximize it with respect to \(\lambda\). 
With some simplification, our new loss function is 
\begin{align*}
    m(\lambda)
    \coloneqq \ell(\mathbf{y} \,|\, \hat{\bm{\beta}}_{\mathrm{MLE}}, \hat{\sigma}^2_{\mathrm{MLE}}, \lambda)
    % \triangleq \ell(\mathbf{y} \,|\, \hat{\bm{\beta}}_{\mathrm{MLE}}, \hat{\sigma}^2_{\mathrm{MLE}}, \lambda)
    = - \frac{n}{2} \log \left( \frac{2 \pi e}{n} \right) - \frac{n}{2} \log \big( \myg^T(\mathbf{I} - \mathbf{H}) \myg \big) + (\lambda - 1) \sum_{i=1}^n \log(y_i).
    % = - \frac{n}{2} \log \left( 2 \pi \mathrm{e} n \right) - \frac{n}{2} \log \big( \myg^T(\mathbf{I} - \mathbf{H}) \myg \big) + (\lambda - 1) \sum_{i=1}^n \log(y_i).
\end{align*}
Ideally, we would differentiate \(m\) with respect to \(\lambda\), set \(\partial m / \partial \lambda = 0\), and solve for \(\lambda\). I was unable to 
derive a closed form solution for the result, but it is still possible to use graphical techniques or numerical methods to find the optimal value of \(\lambda\). 

We recall that both \(\hat{\bm{\beta}}_{\mathrm{MLE}}\) and \(\hat{\sigma}^2_{\mathrm{MLE}}\) are functions of \(\lambda\), so we cannot know their value until \(\hat{\lambda}_{\mathrm{MLE}}\) has been determined.
Because of this, as we just showed, the likelihood function can be expressed as a function \(m(\lambda)\) that only depends on \(\lambda\), which can be maximized to find \(\hat{\lambda}_{\mathrm{MLE}}\). 
Once we find \(\hat{\lambda}_{\mathrm{MLE}}\), we can use this value to determine \(\hat{\bm{\beta}}_{\mathrm{MLE}}\) and \(\hat{\sigma}^2_{\mathrm{MLE}}\). 

% need to add info about transforming model back to y, here and in paragraph below
% maybe put into appendix?

\end{document}