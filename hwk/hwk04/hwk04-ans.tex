\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[margin = 0.75in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}



% \titleformat{\section}{\normalsize\bfseries}{\thesection}{1em}{}
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\setcounter{secnumdepth}{0}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1 \\ }
\newcommand{\mycolaba}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1}

\title{
    {\Large Homework 4}
}
\author{
    {\normalsize Aiden Kenny}\\
    {\normalsize STAT GR5205: Linear Regression Models}\\
    {\normalsize Columbia University}
}
\date{\normalsize November 9, 2020}

\begin{document}

\maketitle

%' ============================================================================================================================================================
\section{Question 1} \noindent



%' ============================================================================================================================================================
\section{Question 2} \noindent



%' ============================================================================================================================================================
\section{Question 3} \noindent



%' ============================================================================================================================================================
\section{Question 4} \noindent
\mycolaba{None}
\begin{itemize}
    \item[(a)] For this question we will assume the matrix \(\mathbf{X}\) is \textsl{centered}, so that each predictor has mean zero. 
    In \(n\)-dimensional Euclidean vector space, for any \(\mathbf{v} \in \mathbb{R}^n\) we have \(\|\mathbf{v}\|^2 = \mathbf{v}^T\mathbf{v}\), 
    so the ridge regression loss function becomes 
    \begin{align*}
        Q(\bm{\beta}; \lambda) 
        = \| \mathbf{y} - \mathbf{X}\bm{\beta} \|^2 + \lambda \|\bm{\beta}\|^2
        = \mathbf{y}^T\mathbf{y} - 2 \bm{\beta}\mathbf{X}^T\mathbf{y} + \bm{\beta} \mathbf{X}^T \mathbf{X} \bm{\beta} + \lambda \bm{\beta}^T \bm{\beta}.
    \end{align*}
    Differentiating the loss function with respect to \(\bm{\beta}\) gives us 
    \begin{align*}
        \frac{\partial Q}{\partial \bm{\beta}}
        = -2 \mathbf{X}^T \mathbf{y} + 2 \mathbf{X}^T\mathbf{X}\bm{\beta} + 2 \lambda \bm{\beta}
        = -2 \mathbf{X}^T \mathbf{y} + 2 \big( \mathbf{X}^T\mathbf{X} + \lambda \mathbf{I} \big) \bm{\beta}
        \overset{\text{set}}{=} \mathbf{0},
    \end{align*}
    and finally solving for \(\bm{\beta}\) gives us 
    \(\hat{\bm{\beta}}_{\text{RR}} = \big( \mathbf{X}^T\mathbf{X} + \lambda \mathbf{I} \big)^{-1} \mathbf{X}^T\mathbf{y}\).
\end{itemize}

\end{document}