\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[top = 1.0in, left = 1.75in, right = 0.75in, bottom = 0.75in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}

\usepackage[explicit]{titlesec}
\titleformat{\section}[runin]{\bfseries}{}{0em}{
    \llap{
        \smash{
            \begin{tabularx}{0.75in}[t]{@{}l@{\hskip0.4em}>{\raggedright}X@{\hskip\marginparsep}}
                #1 
            \end{tabularx}
        }
    }
}[\leavevmode\hspace*{\dimexpr-\fontdimen2\font-\fontdimen3\font+0.25em}]

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\renewcommand{\headrulewidth}{0pt}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1\\}
\newcommand{\mycolaba}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1}

%' ============================================================================================================================================================
%' ============================================================================================================================================================

\begin{document}

\newcommand{\mytitle}{Homework 2}
\newcommand{\myauthor}{Aiden Kenny}
\newcommand{\myclass}{STAT GR5205: Linear Regression Models}
\newcommand{\myschool}{Columbia University}
\newcommand{\mydate}{October 5, 2020}
\begin{flushright}
    \textbf{\mytitle}\\[0.5em]
    \textsl{\myauthor}\\
    \textsl{\myclass}\\
    \textsl{\myschool}\\
    \textsl{\mydate}
\end{flushright} \vspace{1em}

%' ============================================================================================================================================================
\section{Question 1} \noindent
\mycolab{None}
Supposed for \(\mathbf{x},\mathbf{y},\bm{\epsilon}, \mathbf{1} \in \mathbb{R}^n\), where \(\mathbf{x},\mathbf{1}\) are \textsl{fixed} vectors and 
\(\mathbf{y},\bm{\epsilon}\) are 
\textsl{random} vectors, the simple linear regression model 
\begin{align*}
    \mathbf{y} = \beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \bm{\epsilon}
\end{align*}
holds, with \(\mathbb{E}[\bm{\epsilon}] = \mathbf{0}\) and \(\mathrm{Var}[\bm{\epsilon}] = \sigma^2 \mathbf{I}\).
% and \(\mathbb{E}[\mathbf{y}] = \mathbb{E}[\beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \bm{\epsilon}] = \beta_0 \mathbf{1} + \beta_1 \mathbf{x}\).
The least-squares estimators are given by
\begin{align*}
    \hat{\beta}_1 = \frac{(\mathbf{x} - \bar{x}\mathbf{1})^T(\mathbf{y} - \bar{y}\mathbf{1})}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2},
    ~~~
    \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x},
    ~~~\text{and}~~~
    \hat{\sigma}^2 = \frac{1}{n-2} \left\| \mathbf{y} - \hat{\beta}_0 \mathbf{1} - \hat{\beta}_1 \mathbf{x} \right\|^2.
\end{align*}

\begin{itemize}
    \item[(a)] We first determine several properties of \(\mathbf{y}\) (a random vector) and \(\bar{y}\) (a random variable). For \(\mathbf{y}\), we have
    \begin{align*}
        \mathbb{E}[\mathbf{y}] &= \mathbb{E}[\beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \bm{\epsilon}] 
        = \beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \mathbb{E}[\bm{\epsilon}] 
        = \beta_0 \mathbf{1} + \beta_1 \mathbf{x}, \\
        \mathrm{Var}[\mathbf{y}] &= \mathrm{Var}[\beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \bm{\epsilon}]
        = \mathbf{0} + \mathbf{0} + \mathrm{Var}[\bm{\epsilon}] 
        = \sigma^2 \mathbf{I}.
    \end{align*}
    That is, for each \(y_i\), we have \(\mathbb{E}[y_i] = \beta_0 + \beta_1 x_i\) and \(\mathrm{Var}[y_i] = \sigma^2\). 
    We also have \(\mathrm{Cov}[y_i,y_j] = 0\) for all \(i \neq j\). 
    For \(\bar{y}\), we have
    \begin{align*}
        \mathbb{E}[\bar{y}] &= \mathbb{E} \left[ \frac{1}{n} \sum_{i=1}^n y_i \right]
        = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[y_i]
        = \frac{1}{n} \sum_{i=1}^n \big( \beta_0 + \beta_1 x_i \big)
        = \beta_0 + \beta_1 \bar{x}, \\
        \mathrm{Var}[\bar{y}] &= \mathrm{Var} \left[ \frac{1}{n} \sum_{i=1}^n y_i \right]
        = \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}[y_i]
        = \frac{1}{n^2} \sum_{i=1}^n \sigma^2
        = \frac{\sigma^2}{n}.
    \end{align*}
    Expanding out \(\hat{\beta}_1\) gives us 
    \begin{align*}
        \hat{\beta}_1 = \frac{(\mathbf{x} - \bar{x}\mathbf{1})^T(\mathbf{y} - \bar{y}\mathbf{1})}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2}
        = \frac{1}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}).
    \end{align*}
    % and taking the expected value yields
    For notational ease, we are going to multiply both sides of this estimate by \(\|\mathbf{x} - \bar{x}\mathbf{1}\|^2\), since it is just a constant. 
    Taking the expected value of \(\|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1\) yields 
    \begin{align*}
        \mathbb{E} &\Big[ \|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1 \Big] = \|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \mathbb{E}[\hat{\beta}_1] 
        = \mathbb{E} \left[ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \right] \\ 
        &= \sum_{i=1}^n \mathbb{E} \big[ (x_i - \bar{x})(y_i - \bar{y}) \big] 
        = \sum_{i=1}^n (x_i - \bar{x}) \big( \mathbb{E}[y_i] - \mathbb{E}[\bar{y}] \big) \\
        &= \sum_{i=1}^n (x_i - \bar{x}) \big( \beta_0 + \beta_1 x_i - \beta_0 - \beta_1 \bar{x} \big)
        = \beta_1 \sum_{i=1}^n (x_i - \bar{x})^2 
        = \beta_1 \cdot \|\mathbf{x} - \bar{x}\mathbf{1}\|^2.
    \end{align*}
    Dividing both sides of the equation shows that \(\mathbb{E}[\hat{\beta}_1] = \beta_1\).
    Next, taking the expected value of \(\hat{\beta}_0\) gives us 
    \begin{align*}
        \mathbb{E}[\hat{\beta}_0] = \mathbb{E} \big[ \bar{y} - \hat{\beta}_1 \bar{x} \big] 
        = \mathbb{E}[\bar{y}] - \bar{x}\mathbb{E}[\hat{\beta}_1] 
        = \beta_0 + \beta_1 \bar{x} - \beta_1 \bar{x}
        = \beta_0.
    \end{align*}
    Finally, taking the expected value of \(\hat{\sigma}^2\) leads to 
    % ...
    \item[(b)] Looking at the expanded equation for \(\hat{\beta}_1\), we have 
    \begin{align*}
        \|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1 &= \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
        = \sum_{i=1}^n (x_i - \bar{x}) y_i - \bar{y} \sum_{i=1}^n (x_i - \bar{x}) \\
        &= \sum_{i=1}^n (x_i - \bar{x}) y_i - \bar{y} \big( n \bar{x} - n \bar x \big)
        = \sum_{i=1}^n (x_i - \bar{x}) y_i.
    \end{align*}
    That is, we are able to remove the \(\bar{y}\) from the summation entirely. By taking the variance of 
    \(\|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1\), we have 
    \begin{align*}
        \mathrm{Var} &\Big[ \|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1 \Big]
        = \|\mathbf{x} - \bar{x}\mathbf{1}\|^4 \cdot \mathrm{Var}[\hat{\beta}_1]
        = \mathrm{Var} \left[ \sum_{i=1}^n (x_i - \bar{x}) y_i \right] \\
        &= \sum_{i=1}^n \mathrm{Var}\big[(x_i - \bar{x}) y_i\big]
        = \sum_{i=1}^n (x_i - \bar{x})^2 \mathrm{Var}[y_i]
        = \sum_{i=1}^n (x_i - \bar{x})^2 \sigma^2
        = \sigma^2 \cdot \|\mathbf{x} - \bar{x}\mathbf{1}\|^2,
    \end{align*}
    and dividing both sides by \(\|\mathbf{x} - \bar{x}\mathbf{1}\|^2\) shows that 
    \(\mathrm{Var}[\hat{\beta}_1] = \sigma^2 / \|\mathbf{x} - \bar{x}\mathbf{1}\|^2\). 
    Similarly, taking the variance of \(\hat{\beta}_0\) gives us 
    \begin{align*}
        \mathrm{Var}[\hat{\beta}_0] = \mathrm{Var} \big[ \bar{y} - \hat{\beta}_1 \bar{x} \big]
        = \mathrm{Var}[\bar{y}] + \bar{x}^2 \mathrm{Var}[\hat{\beta}_1]
        = \frac{\sigma^2}{n} + \bar{x}^2\frac{\sigma^2}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2}
        = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2} \right).
    \end{align*}
\end{itemize}

%' ============================================================================================================================================================
\section{Question 2} \noindent
\mycolab{None}
Letting \(\bm{\beta} = (\beta_0, \beta_1)^T\) and \(\mathbf{X} = \begin{bmatrix}
    \mathbf{1} & \mathbf{x}
\end{bmatrix} \in \mathbb{R}^{n\times2}\), the simple linear regression model is given by \(\mathbf{y} = \mathbf{X}\bm{\beta} + \bm{\epsilon}\). 
The MSE is then given by \(Q = \| \mathbf{y} - \mathbf{X}\bm{\beta} \|^2\). 
\begin{itemize}
    \item[(a)] We first expand the MSE to get
    \begin{align*}
        Q = \| \mathbf{y} - \mathbf{X}\bm{\beta} \|^2 = (\mathbf{y} - \mathbf{X}\bm{\beta})^T(\mathbf{y} - \mathbf{X}\bm{\beta})
        = \mathbf{y}^T\mathbf{y} - 2 \mathbf{y}^T \mathbf{X} \bm{\beta} + \bm{\beta}^T \mathbf{X}^T \mathbf{X} \bm{\beta}.
    \end{align*}
    By differentiating \(Q\) with respect to \(\bm{\beta}\) and setting it equal to \(\mathbf{0}\), we have
    \begin{align*}
        \frac{\partial Q}{\partial\bm{\beta}} 
        &= \frac{\partial}{\partial\bm{\beta}} \left( \mathbf{y}^T\mathbf{y} - 2 \mathbf{y}^T \mathbf{X} \bm{\beta} + \bm{\beta}^T \mathbf{X}^T \mathbf{X} \bm{\beta} \right) \\
        &= \frac{\partial}{\partial\bm{\beta}} \mathbf{y}^T\mathbf{y} - 2 \frac{\partial}{\partial\bm{\beta}}\mathbf{y}^T \mathbf{X} \bm{\beta} + \frac{\partial}{\partial\bm{\beta}}\bm{\beta}^T \mathbf{X}^T \mathbf{X} \bm{\beta}
        = -2 \mathbf{X}^T \mathbf{y} + 2 \mathbf{X}^T \mathbf{X} \bm{\beta}
        ~\overset{\text{set}}{=}~ \mathbf{0},
    \end{align*}
    and solving for \(\bm{\beta}\) gives us \(\hat{\bm{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\).
    \item[(b)] Now suppose that we have \(p\) different predictors, each with \(n\) observed values that are not all identical. 
    Let \(\mathbf{x}_j \in \mathbb{R}^n\) be the vector containing the observations for the \(j\)th predictor. Define the matrix
    \(\mathbf{X} = \begin{bmatrix}
        \mathbf{1} & \mathbf{x}_1 & \cdots & \mathbf{x}_p
    \end{bmatrix} \in \mathbb{R}^{n\times(p+1)}\) be the matrix whos \(j\)th column is \(\mathbf{x}_{j-1}\) (and first column is \(\mathbf{1}\)).
    In addition, let \(\bm{\beta} = (\beta_0, \beta_1, \ldots, \beta_p)^T \in \mathbb{R}^{p+1}\) be the vector containing \(p+1\) scalars. 
    The multiple linear regression model is given by \(\mathbf{y} = \mathbf{X}\bm{\beta} + \bm{\epsilon}\); that is, it has the same form as the 
    simple linear regression model, and we can see that simple linear regression is when \(p=1\). Because of this, the estimated 
    coefficients \(\hat{\bm{\beta}}\) take the same form as before: \(\hat{\bm{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\).
    \item[(c)] The fitted values \(\hat{\mathbf{y}}\) are given by 
    \begin{align*}
        \hat{\mathbf{y}} = \mathbf{X}\hat{\bm{\beta}} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.
    \end{align*}
    \item[(d)] Similar to simple linear regression, the ``normal equation'' for the multivariate regression setting is found during the derivation of 
    \(\hat{\bm{\beta}}\), and is given by 
    \begin{align*}
        \mathbf{X}^T (\mathbf{y} - \mathbf{X}\hat{\bm{\beta}}) = \mathbf{0}.
    \end{align*}
    % An immediate consequence of this are results (a) and (c): 
    Letting \(\mathbf{e} = \mathbf{y} - \mathbf{X}\hat{\bm{\beta}}\) be the vector of observed residuals,
    we have \(\mathbf{X}^T\mathbf{e} = \mathbf{0}\), meaning
    \(\mathbf{1}^T \mathbf{e} = 0\) and \(\mathbf{x}_j^T \mathbf{e} = 0\) for all \(j\). In other words, the residuals sum to zero and the residuals
    weighted by each predictor sum to 0. Because of this result, we also know that the residuals weighted by the fitted values sum to zero, i.e.
    \begin{align*}
        \hat{\mathbf{y}}^T \mathbf{e} = (\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y})^T\mathbf{e}
        = \mathbf{y}^T \mathbf{X} (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \mathbf{e}
        = \mathbf{y}^T \mathbf{X} (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{0}
        = \mathbf{0}.
    \end{align*}
    The other key results are that \(\hat{\mathbf{y}}^T \mathbf{1} = \mathbf{y}^T\mathbf{1}\) and \(\bar{y} = \bar{\mathbf{x}}^T \hat{\bm{\beta}}\),
    where \(\bar{\mathbf{x}} = (1, \bar{x}_1, \ldots, \bar{x}_p)^T\) is the vector whos \(j\)th entry is the mean of the \((j-1)\)th variable. 
    \item[(e)] We have 
    \begin{align*}
        \| \hat{\mathbf{y}} - \bar{y}\mathbf{1} \|^2 &= (\hat{\mathbf{y}} - \bar{y}\mathbf{1})^T(\hat{\mathbf{y}} - \bar{y}\mathbf{1}) \\
        &= \hat{\mathbf{y}}^T \hat{\mathbf{y}} - \bar{y}\hat{\mathbf{y}}^T\mathbf{1} - \bar{y}\hat{\mathbf{y}}^T\mathbf{1} + \bar{y}^2 \mathbf{1}^T\mathbf{1} \\
        &= \hat{\mathbf{y}}^T \hat{\mathbf{y}} - \bar{y}\mathbf{y}^T\mathbf{1} - \bar{y}\hat{\mathbf{y}}^T\mathbf{1} + \bar{y}^2 \mathbf{1}^T\mathbf{1} \tag{since \(\hat{\mathbf{y}}^T\mathbf{1} = \mathbf{y}^T\mathbf{1}\)}\\
        &= \hat{\mathbf{y}}^T (\mathbf{y} - \mathbf{e}) - \bar{y}\mathbf{y}^T\mathbf{1} - \bar{y}\hat{\mathbf{y}}^T\mathbf{1} + \bar{y}^2 \mathbf{1}^T\mathbf{1} \tag{since \(\hat{\mathbf{y}} = \mathbf{y} - \mathbf{e}\)}\\
        &= \hat{\mathbf{y}}^T\mathbf{y} - \hat{\mathbf{y}}^T\mathbf{e} - \bar{y}\mathbf{y}^T\mathbf{1} - \bar{y}\hat{\mathbf{y}}^T\mathbf{1} + \bar{y}^2 \mathbf{1}^T\mathbf{1} \\
        &= \hat{\mathbf{y}}^T\mathbf{y} - \bar{y}\mathbf{y}^T\mathbf{1} - \bar{y}\hat{\mathbf{y}}^T\mathbf{1} + \bar{y}^2 \mathbf{1}^T\mathbf{1} \tag{since \(\hat{\mathbf{y}}^T\mathbf{e} = \mathbf{0}\)} \\
        &= (\hat{\mathbf{y}} - \bar{y}\mathbf{1})^T(\mathbf{y} - \bar{y}\mathbf{1}).
    \end{align*}
    Essentially, we expanded out the equation and used the key results from part (d) to mainpulate the equation and get the desired result. 
\end{itemize}

%' ============================================================================================================================================================
\section{Question 3} \noindent
\mycolaba{None}
\begin{itemize}
    \item[(a)] Our hypothesis test is given by \(H_0:\beta_1 \le 0\) vs. \(H_a:\beta_1 > 0\), and we fail to reject \(H_0\). The analyst claiming that 
    there is no linear association between \(X\) and \(Y\) is \textsl{incorrect}. The data indicates that \(\beta_1 \le 0\), not \(\beta_1 = 1\), meaning
    there could be a negative linear association. 
    \item[(b)] While this situation is more subtle, the analyst is again incorrect. In the first situation, we are estimating the true value of 
    \(\mathbb{E}[Y|X=x_0]\), so we are trying to estimate an underlying feature of the random variable \(Y\). In the second situation, we are trying 
    to predict the average of \textsl{random samples} from \(Y\), i.e. we want to estimate \(\mathbb{E}[\bar{Y}|X=x_0]\). One reason for this confusion is that
    the two results are the same. For example, if \(Z \sim \mathrm{N}(\mu, \sigma^2)\), then \(\bar{Z}\sim\mathrm{N}(\mu,\sigma^2/n)\), which means 
    \(\mathbb{E}[Z] = \mathbb{E}[\bar{Z}] = \mu\). It is important to note that even though the numerical answer is the same, \textsl{what} we are doing in
    each situation is different.
    \item[(c)] At the point \(X = x_0\), a \(95\%{}\) confidence interval for \(Y_0\) is given by 
    \begin{align*}
        \hat{Y}_0 \pm t_{0.975,n-2} \sqrt{\mathrm{MSE} \left( \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\sum_{i=1}^n(x_i - \bar{x})^2} \right)}.
    \end{align*}
    However, this confidence interval would \textsl{not} be the same at some different point \(X = x_h \neq x_0\). At this point, a \(95\%{}\)
    confidence interval would be given by 
    \begin{align*}
        \hat{Y}_h \pm t_{0.975,n-2} \sqrt{\mathrm{MSE} \left( \frac{1}{n} + \frac{(x_h - \bar{x})^2}{\sum_{i=1}^n(x_i - \bar{x})^2} \right)}.
    \end{align*}
    This new confidence interval is based around \(\hat{Y}_h\), not \(\hat{Y}_0\), and its width is based on \(x_h\), not \(x_0\).
\end{itemize}

%' ============================================================================================================================================================
\section{Question 4} \noindent
\mycolab{None}


\end{document}