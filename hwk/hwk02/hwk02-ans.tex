\documentclass[10pt]{article}

\usepackage{mathtools, amssymb, bm}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[top = 1.0in, left = 1.75in, right = 0.75in, bottom = 0.75in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{tikzsymbols}
\usepackage[hidelinks]{hyperref}

\usepackage[explicit]{titlesec}
\titleformat{\section}[runin]{\bfseries}{}{0em}{
    \llap{
        \smash{
            \begin{tabularx}{0.75in}[t]{@{}l@{\hskip0.4em}>{\raggedright}X@{\hskip\marginparsep}}
                #1 
            \end{tabularx}
        }
    }
}[\leavevmode\hspace*{\dimexpr-\fontdimen2\font-\fontdimen3\font+0.25em}]

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\renewcommand{\headrulewidth}{0pt}

\definecolor{colabcol}{HTML}{960018}
\newcommand{\mycolab}[1]{\textcolor{colabcol}{\textsl{Collaborators:}} #1\\}

%' ============================================================================================================================================================
%' ============================================================================================================================================================

\begin{document}

\newcommand{\mytitle}{Homework 2}
\newcommand{\myauthor}{Aiden Kenny}
\newcommand{\myclass}{STAT GR5205: Linear Regression Models}
\newcommand{\myschool}{Columbia University}
\newcommand{\mydate}{October 5, 2020}
\begin{flushright}
    \textbf{\mytitle}\\[0.5em]
    \textsl{\myauthor}\\
    \textsl{\myclass}\\
    \textsl{\myschool}\\
    \textsl{\mydate}
\end{flushright} \vspace{1em}

%' ============================================================================================================================================================
\section{Question 1} \noindent
\mycolab{None}
Supposed for \(\mathbf{x},\mathbf{y},\bm{\epsilon}, \mathbf{1} \in \mathbb{R}^n\), where \(\mathbf{x},\mathbf{1}\) are \textsl{fixed} vectors and 
\(\mathbf{y},\bm{\epsilon}\) are 
\textsl{random} vectors, the simple linear regression model 
\begin{align*}
    \mathbf{y} = \beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \bm{\epsilon}
\end{align*}
holds, with \(\mathbb{E}[\bm{\epsilon}] = \mathbf{0}\) and \(\mathrm{Var}[\bm{\epsilon}] = \sigma^2 \mathbf{I}\).
% and \(\mathbb{E}[\mathbf{y}] = \mathbb{E}[\beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \bm{\epsilon}] = \beta_0 \mathbf{1} + \beta_1 \mathbf{x}\).
The least-squares estimators are given by
\begin{align*}
    \hat{\beta}_1 = \frac{(\mathbf{x} - \bar{x}\mathbf{1})^T(\mathbf{y} - \bar{y}\mathbf{1})}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2},
    ~~~
    \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x},
    ~~~\text{and}~~~
    \hat{\sigma}^2 = \frac{1}{n-2} \left\| \mathbf{y} - \hat{\beta}_0 \mathbf{1} - \hat{\beta}_1 \mathbf{x} \right\|^2.
\end{align*}

\begin{itemize}
    \item[(a)] We first determine several properties of \(\mathbf{y}\) (a random vector) and \(\bar{y}\) (a random variable). For \(\mathbf{y}\), we have
    \begin{align*}
        \mathbb{E}[\mathbf{y}] &= \mathbb{E}[\beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \bm{\epsilon}] 
        = \beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \mathbb{E}[\bm{\epsilon}] 
        = \beta_0 \mathbf{1} + \beta_1 \mathbf{x}, \\
        \mathrm{Var}[\mathbf{y}] &= \mathrm{Var}[\beta_0 \mathbf{1} + \beta_1 \mathbf{x} + \bm{\epsilon}]
        = \mathbf{0} + \mathbf{0} + \mathrm{Var}[\bm{\epsilon}] 
        = \sigma^2 \mathbf{I}.
    \end{align*}
    That is, for each \(y_i\), we have \(\mathbb{E}[y_i] = \beta_0 + \beta_1 x_i\) and \(\mathrm{Var}[y_i] = \sigma^2\). 
    We also have \(\mathrm{Cov}[y_i,y_j] = 0\) for all \(i \neq j\). 
    For \(\bar{y}\), we have
    \begin{align*}
        \mathbb{E}[\bar{y}] &= \mathbb{E} \left[ \frac{1}{n} \sum_{i=1}^n y_i \right]
        = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[y_i]
        = \frac{1}{n} \sum_{i=1}^n \big( \beta_0 + \beta_1 x_i \big)
        = \beta_0 + \beta_1 \bar{x}, \\
        \mathrm{Var}[\bar{y}] &= \mathrm{Var} \left[ \frac{1}{n} \sum_{i=1}^n y_i \right]
        = \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}[y_i]
        = \frac{1}{n^2} \sum_{i=1}^n \sigma^2
        = \frac{\sigma^2}{n}.
    \end{align*}
    Expanding out \(\hat{\beta}_1\) gives us 
    \begin{align*}
        \hat{\beta}_1 = \frac{(\mathbf{x} - \bar{x}\mathbf{1})^T(\mathbf{y} - \bar{y}\mathbf{1})}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2}
        = \frac{1}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}).
    \end{align*}
    % and taking the expected value yields
    For notational ease, we are going to multiply both sides of this estimate by \(\|\mathbf{x} - \bar{x}\mathbf{1}\|^2\), since it is just a constant. 
    Taking the expected value of \(\|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1\) yields 
    \begin{align*}
        \mathbb{E} &\Big[ \|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1 \Big] = \|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \mathbb{E}[\hat{\beta}_1] 
        = \mathbb{E} \left[ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \right] \\ 
        &= \sum_{i=1}^n \mathbb{E} \big[ (x_i - \bar{x})(y_i - \bar{y}) \big] 
        = \sum_{i=1}^n (x_i - \bar{x}) \big( \mathbb{E}[y_i] - \mathbb{E}[\bar{y}] \big) \\
        &= \sum_{i=1}^n (x_i - \bar{x}) \big( \beta_0 + \beta_1 x_i - \beta_0 - \beta_1 \bar{x} \big)
        = \beta_1 \sum_{i=1}^n (x_i - \bar{x})^2 
        = \beta_1 \cdot \|\mathbf{x} - \bar{x}\mathbf{1}\|^2.
    \end{align*}
    Dividing both sides of the equation shows that \(\mathbb{E}[\hat{\beta}_1] = \beta_1\).
    Next, taking the expected value of \(\hat{\beta}_0\) gives us 
    \begin{align*}
        \mathbb{E}[\hat{\beta}_0] = \mathbb{E} \big[ \bar{y} - \hat{\beta}_1 \bar{x} \big] 
        = \mathbb{E}[\bar{y}] - \bar{x}\mathbb{E}[\hat{\beta}_1] 
        = \beta_0 + \beta_1 \bar{x} - \beta_1 \bar{x}
        = \beta_0.
    \end{align*}
    Finally, taking the expected value of \(\hat{\sigma}^2\) leads to 
    % ...
    \item[(b)] Looking at the expanded equation for \(\hat{\beta}_1\), we have 
    \begin{align*}
        \|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1 &= \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
        = \sum_{i=1}^n (x_i - \bar{x}) y_i - \bar{y} \sum_{i=1}^n (x_i - \bar{x}) \\
        &= \sum_{i=1}^n (x_i - \bar{x}) y_i - \bar{y} \big( n \bar{x} - n \bar x \big)
        = \sum_{i=1}^n (x_i - \bar{x}) y_i.
    \end{align*}
    That is, we are able to remove the \(\bar{y}\) from the summation entirely. By taking the variance of 
    \(\|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1\), we have 
    \begin{align*}
        \mathrm{Var} &\Big[ \|\mathbf{x} - \bar{x}\mathbf{1}\|^2 \cdot \hat{\beta}_1 \Big]
        = \|\mathbf{x} - \bar{x}\mathbf{1}\|^4 \cdot \mathrm{Var}[\hat{\beta}_1]
        = \mathrm{Var} \left[ \sum_{i=1}^n (x_i - \bar{x}) y_i \right] \\
        &= \sum_{i=1}^n \mathrm{Var}\big[(x_i - \bar{x}) y_i\big]
        = \sum_{i=1}^n (x_i - \bar{x})^2 \mathrm{Var}[y_i]
        = \sum_{i=1}^n (x_i - \bar{x})^2 \sigma^2
        = \sigma^2 \cdot \|\mathbf{x} - \bar{x}\mathbf{1}\|^2,
    \end{align*}
    and dividing both sides by \(\|\mathbf{x} - \bar{x}\mathbf{1}\|^2\) shows that 
    \(\mathrm{Var}[\hat{\beta}_1] = \sigma^2 / \|\mathbf{x} - \bar{x}\mathbf{1}\|^2\). 
    Similarly, taking the variance of \(\hat{\beta}_0\) gives us 
    \begin{align*}
        \mathrm{Var}[\hat{\beta}_0] = \mathrm{Var} \big[ \bar{y} - \hat{\beta}_1 \bar{x} \big]
        = \mathrm{Var}[\bar{y}] + \bar{x}^2 \mathrm{Var}[\hat{\beta}_1]
        = \frac{\sigma^2}{n} + \bar{x}^2\frac{\sigma^2}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2}
        = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}}{\|\mathbf{x} - \bar{x}\mathbf{1}\|^2} \right).
    \end{align*}
\end{itemize}

%' ============================================================================================================================================================
\section{Question 2} \noindent
\mycolab{None}
stop

\end{document}